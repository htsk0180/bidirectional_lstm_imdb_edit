{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AuiS3XG59UP"
      },
      "source": [
        "# IMDB verisi üzerinde Bidirectional LSTM'in Basit Bir Uygulaması\n",
        "\n",
        "Bu notebook'da IMDB film inceleme duyarlılığı sınıflandırma veri kümesinde 2 katmanlı çift yönlü bir LSTM eğiteceğiz."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyGhLdcL59US"
      },
      "source": [
        "## Kurulum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "z5lumCmD59UT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "max_features = 20000  # Yalnızca ilk 20 bin kelimeyi dikkate alın\n",
        "maxlen = 200  # Her film incelemesinin yalnızca ilk 200 kelimesini dikkate alın\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcl6Jf8B59UU"
      },
      "source": [
        "## Modeli oluşturalım"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qP7Gn3Sk59UU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e4932d4-9b63-4f95-cf94-242c610bea52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, None, 128)         2560000   \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, None, 128)        98816     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 128)              98816     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,757,761\n",
            "Trainable params: 2,757,761\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Değişken uzunluklu tamsayı dizileri için girdi\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "# Her tamsayıyı 128 boyutlu bir vektöre gömün\n",
        "x = layers.Embedding(max_features, 128)(inputs)\n",
        "# 2 çift yönlü LSTM ekleyin\n",
        "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
        "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
        "# Sınıflandırıcı ekle\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwg8D3ru59UU"
      },
      "source": [
        "## IMDB film incelemesi için duyarlılık verilerini yükleyin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PzqNl5l959UV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a7fb243-4b4a-4195-e575-56e61f251447"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25000 Eğitim dizileri\n",
            "25000 Doğrulama dizileri\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(\n",
        "    num_words=max_features\n",
        ")\n",
        "print(len(x_train), \"Eğitim dizileri\")\n",
        "print(len(x_val), \"Doğrulama dizileri\")\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQGLmuxi59UV"
      },
      "source": [
        "## Modeli eğitin ve değerlendirin\n",
        "\n",
        "Ayrıca [Hugging Face Hub](https://huggingface.co/keras-io/bidirectional-lstm-imdb) üzerinde barındırılan eğitimli modeli kullanabilirdi.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDR0QadI59UV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6cb7e8d0-bd94-4f37-a544-52022d7ad8ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "782/782 [==============================] - 111s 126ms/step - loss: 0.3809 - accuracy: 0.8272 - val_loss: 0.3284 - val_accuracy: 0.8629\n",
            "Epoch 2/2\n",
            "782/782 [==============================] - 44s 57ms/step - loss: 0.1963 - accuracy: 0.9267 - val_loss: 0.3373 - val_accuracy: 0.8545\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc0b2c7d4c0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model parametrelerini optimize edelim"
      ],
      "metadata": {
        "id": "JOtES2xl8lJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# model parametrelerin optimize edilmesi.\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "def create_model(optimizer='adam'):\n",
        "    model = keras.Sequential()\n",
        "    model.add(layers.Embedding(max_features, 128))\n",
        "    model.add(layers.Bidirectional(layers.LSTM(64, return_sequences=True)))\n",
        "    model.add(layers.Bidirectional(layers.LSTM(64)))\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "batch_size = [10, 20, 40, 60, 80, 100]\n",
        "epochs = [2, 3, 5, 10]\n",
        "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
        "param_grid = dict(batch_size=batch_size, epochs=epochs, optimizer=optimizer)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(x_train, y_train)\n",
        "\n",
        "# en iyi parametreler\n",
        "print(\"En iyi parametreler: %s with a score of %f\" % (grid_result.best_params_, grid_result.best_score_))\n",
        "\n",
        "# en iyi parametreler ile modelin yeniden eğitilmesi\n",
        "model = create_model(optimizer = 'Adam')\n",
        "model.fit(x_train, y_train, batch_size=100, epochs=10, validation_data=(x_val, y_val))\n",
        "\n",
        "# modelin test verileri ile test edilmesi\n",
        "model.evaluate(x_val, y_val)"
      ],
      "metadata": {
        "id": "ODrUr1GW7q6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aynı proje üzerinden mimari sabit kalarak optimizer'in değiştirilmesi ( Lion )"
      ],
      "metadata": {
        "id": "-WFA840c_-V7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"TF2 implementation of the Lion optimizer.\"\"\"\n",
        "\n",
        "import tensorflow.compat.v2 as tf\n",
        "\n",
        "\n",
        "class Lion(tf.keras.optimizers.legacy.Optimizer):\n",
        "  r\"\"\"Optimizer that implements the Lion algorithm.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               learning_rate=0.0001,\n",
        "               beta_1=0.9,\n",
        "               beta_2=0.99,\n",
        "               wd=0,\n",
        "               name='lion',\n",
        "               **kwargs):\n",
        "    \"\"\"Construct a new Lion optimizer.\"\"\"\n",
        "\n",
        "    super(Lion, self).__init__(name, **kwargs)\n",
        "    self._set_hyper('learning_rate', kwargs.get('lr', learning_rate))\n",
        "    self._set_hyper('beta_1', beta_1)\n",
        "    self._set_hyper('beta_2', beta_2)\n",
        "    self._set_hyper('wd', wd)\n",
        "\n",
        "  def _create_slots(self, var_list):\n",
        "    # Create slots for the first and second moments.\n",
        "    # Separate for-loops to respect the ordering of slot variables from v1.\n",
        "    for var in var_list:\n",
        "      self.add_slot(var, 'm')\n",
        "\n",
        "  def _prepare_local(self, var_device, var_dtype, apply_state):\n",
        "    super(Lion, self)._prepare_local(var_device, var_dtype, apply_state)\n",
        "\n",
        "    beta_1_t = tf.identity(self._get_hyper('beta_1', var_dtype))\n",
        "    beta_2_t = tf.identity(self._get_hyper('beta_2', var_dtype))\n",
        "    wd_t = tf.identity(self._get_hyper('wd', var_dtype))\n",
        "    lr = apply_state[(var_device, var_dtype)]['lr_t']\n",
        "    apply_state[(var_device, var_dtype)].update(\n",
        "        dict(\n",
        "            lr=lr,\n",
        "            beta_1_t=beta_1_t,\n",
        "            one_minus_beta_1_t=1 - beta_1_t,\n",
        "            beta_2_t=beta_2_t,\n",
        "            one_minus_beta_2_t=1 - beta_2_t,\n",
        "            wd_t=wd_t))\n",
        "\n",
        "  @tf.function(jit_compile=True)\n",
        "  def _resource_apply_dense(self, grad, var, apply_state=None):\n",
        "    var_device, var_dtype = var.device, var.dtype.base_dtype\n",
        "    coefficients = ((apply_state or {}).get((var_device, var_dtype)) or\n",
        "                    self._fallback_apply_state(var_device, var_dtype))\n",
        "\n",
        "    m = self.get_slot(var, 'm')\n",
        "    var_t = var.assign_sub(\n",
        "        coefficients['lr_t'] *\n",
        "        (tf.math.sign(m * coefficients['beta_1_t'] +\n",
        "                      grad * coefficients['one_minus_beta_1_t']) +\n",
        "         var * coefficients['wd_t']))\n",
        "    with tf.control_dependencies([var_t]):\n",
        "      m.assign(m * coefficients['beta_2_t'] +\n",
        "               grad * coefficients['one_minus_beta_2_t'])\n",
        "\n",
        "  @tf.function(jit_compile=True)\n",
        "  def _resource_apply_sparse(self, grad, var, indices, apply_state=None):\n",
        "    var_device, var_dtype = var.device, var.dtype.base_dtype\n",
        "    coefficients = ((apply_state or {}).get((var_device, var_dtype)) or\n",
        "                    self._fallback_apply_state(var_device, var_dtype))\n",
        "\n",
        "    m = self.get_slot(var, 'm')\n",
        "    m_t = m.assign(m * coefficients['beta_1_t'])\n",
        "    m_scaled_g_values = grad * coefficients['one_minus_beta_1_t']\n",
        "    m_t = m_t.scatter_add(tf.IndexedSlices(m_scaled_g_values, indices))\n",
        "    var_t = var.assign_sub(coefficients['lr'] *\n",
        "                           (tf.math.sign(m_t) + var * coefficients['wd_t']))\n",
        "\n",
        "    with tf.control_dependencies([var_t]):\n",
        "      m_t = m_t.scatter_add(tf.IndexedSlices(-m_scaled_g_values, indices))\n",
        "      m_t = m_t.assign(m_t * coefficients['beta_2_t'] /\n",
        "                       coefficients['beta_1_t'])\n",
        "      m_scaled_g_values = grad * coefficients['one_minus_beta_2_t']\n",
        "      m_t.scatter_add(tf.IndexedSlices(m_scaled_g_values, indices))\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super(Lion, self).get_config()\n",
        "    config.update({\n",
        "        'learning_rate': self._serialize_hyperparameter('learning_rate'),\n",
        "        'beta_1': self._serialize_hyperparameter('beta_1'),\n",
        "        'beta_2': self._serialize_hyperparameter('beta_2'),\n",
        "        'wd': self._serialize_hyperparameter('wd'),\n",
        "    })\n",
        "    return config\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "max_features = 20000  # Yalnızca ilk 20 bin kelimeyi dikkate alın\n",
        "maxlen = 200  # Her film incelemesinin yalnızca ilk 200 kelimesini dikkate alın\n",
        "# Değişken uzunluklu tamsayı dizileri için girdi\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int32\")\n",
        "# Her tamsayıyı 128 boyutlu bir vektöre gömün\n",
        "x = layers.Embedding(max_features, 128)(inputs)\n",
        "# 2 çift yönlü LSTM ekleyin\n",
        "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
        "x = layers.Bidirectional(layers.LSTM(64))(x)\n",
        "# Sınıflandırıcı ekle\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.summary()\n",
        "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(\n",
        "    num_words=max_features\n",
        ")\n",
        "print(len(x_train), \"Eğitim dizileri\")\n",
        "print(len(x_val), \"Doğrulama dizileri\")\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)\n",
        "\n",
        "#model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "#model.fit(x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val))\n",
        "# adam optimizer yerine Lion optimizer kullanıldı\n",
        "model.compile(Lion(), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=5, validation_data=(x_val, y_val))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2QYgesIATIE",
        "outputId": "2e0cab28-c0a8-4c71-ccf1-220cb93cb9ed"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, None, 128)         2560000   \n",
            "                                                                 \n",
            " bidirectional_4 (Bidirectio  (None, None, 128)        98816     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " bidirectional_5 (Bidirectio  (None, 128)              98816     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,757,761\n",
            "Trainable params: 2,757,761\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "25000 Eğitim dizileri\n",
            "25000 Doğrulama dizileri\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 68s 79ms/step - loss: 0.4302 - accuracy: 0.7966 - val_loss: 0.3189 - val_accuracy: 0.8646\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 36s 46ms/step - loss: 0.1833 - accuracy: 0.9332 - val_loss: 0.3996 - val_accuracy: 0.8545\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 34s 44ms/step - loss: 0.0886 - accuracy: 0.9703 - val_loss: 0.4213 - val_accuracy: 0.8453\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 34s 43ms/step - loss: 0.0534 - accuracy: 0.9834 - val_loss: 0.5486 - val_accuracy: 0.8400\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 32s 41ms/step - loss: 0.0412 - accuracy: 0.9864 - val_loss: 0.5878 - val_accuracy: 0.8349\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f11803408e0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}